{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data preperation\n",
    "# Preparing datasets for further using\n",
    "# Preprocessing loaded MNIST datasets for further using in classifier\n",
    "# Saving datasets into file\n",
    "\n",
    "\n",
    "\"\"\"Importing library for object serialization\n",
    "which we'll use for saving and loading serialized models\"\"\"\n",
    "import pickle\n",
    "\n",
    "# Importing other standard libraries\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Defining function for loading MNIST images\n",
    "def load_data(file, number_of_images):\n",
    "    # Opening file for reading in binary mode\n",
    "    with gzip.open(file) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        \"\"\"Initially testing file with images has shape (60000 * 784)\n",
    "        Where, 60000 - number of image samples\n",
    "        784 - one channel of image (28 x 28)\n",
    "        Every image consists of 28x28 pixels with its only one channel\"\"\"\n",
    "        # Reading data\n",
    "        buf = bytestream.read(number_of_images * 28 * 28)\n",
    "        # Placing data in numpy array and converting it into 'float32' type\n",
    "        # It is used further in function 'pre_process_mnist' as it is needed to subtract float from float\n",
    "        # And for standard deviation as it is needed to divide float by float\n",
    "        data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "        # Reshaping data making for every image separate matrix (28, 28)\n",
    "        data = data.reshape(number_of_images, 28, 28)  # (60000, 28, 28)\n",
    "\n",
    "        # Preparing array with shape for 1 channeled image\n",
    "        # Making for every image separate matrix (28, 28, 1)\n",
    "        array_of_image = np.zeros((number_of_images, 28, 28, 1))  # (60000, 28, 28, 1)\n",
    "\n",
    "        # Assigning to array one channeled image from dataset\n",
    "        # In this way we get normal 3-channeled images\n",
    "        array_of_image[:, :, :, 0] = data\n",
    "\n",
    "    # Returning array of loaded images from file\n",
    "    return array_of_image\n",
    "\n",
    "\n",
    "# Defining function for loading MNIST labels\n",
    "def load_labels(file, number_of_labels):\n",
    "    # Opening file for reading in binary mode\n",
    "    with gzip.open(file) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        \"\"\"Initially testing file with labels has shape (60000)\n",
    "        Where, 60000 - number of labels\"\"\"\n",
    "        # Reading data\n",
    "        buf = bytestream.read(number_of_labels)\n",
    "        # Placing data in numpy array and converting it into 'int64' type\n",
    "        labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int64)  # (60000, )\n",
    "\n",
    "    # Returning array of loaded labels from file\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Preparing function for preprocessing MNIST datasets for further use in classifier\n",
    "def pre_process_mnist(x_train, y_train, x_test, y_test):\n",
    "    # Normalizing whole data by dividing /255.0\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0  # Data for testing consists of 10000 examples from testing dataset\n",
    "\n",
    "    # Preparing data for training, validation and testing\n",
    "    # Data for validation is taken with 1000 examples from training dataset in range from 59000 to 60000\n",
    "    batch_mask = list(range(59000, 60000))\n",
    "    x_validation = x_train[batch_mask]  # (1000, 28, 28, 1)\n",
    "    y_validation = y_train[batch_mask]  # (1000,)\n",
    "    # Data for training is taken with first 59000 examples from training dataset\n",
    "    batch_mask = list(range(59000))\n",
    "    x_train = x_train[batch_mask]  # (59000, 28, 28, 1)\n",
    "    y_train = y_train[batch_mask]  # (59000,)\n",
    "\n",
    "    # Normalizing data by subtracting mean image and dividing by standard deviation\n",
    "    # Subtracting the dataset by mean image serves to center the data.\n",
    "    # It helps for each feature to have a similar range and gradients don't go out of control.\n",
    "    # Calculating mean image from training dataset along the rows by specifying 'axis=0'\n",
    "    mean_image = np.mean(x_train, axis=0)  # numpy.ndarray (28, 28, 1)\n",
    "\n",
    "    # Calculating standard deviation from training dataset along the rows by specifying 'axis=0'\n",
    "    std = np.std(x_train, axis=0)  # numpy.ndarray (28, 28, 1)\n",
    "    # Taking into account that a lot of values are 0, that is why we need to replace it to 1\n",
    "    # In order to avoid dividing by 0\n",
    "    for j in range(28):\n",
    "        for i in range(28):\n",
    "            if std[i, j, 0] == 0:\n",
    "                std[i, j, 0] = 1.0\n",
    "\n",
    "    # Saving calculated 'mean_image' and 'std' into 'pickle' file\n",
    "    # We will use them when preprocessing input data for classifying\n",
    "    # We will need to subtract and divide input image for classifying\n",
    "    # As we're doing now for training, validation and testing data\n",
    "    dictionary = {'mean_image': mean_image, 'std': std}\n",
    "    with open('mean_and_std.pickle', 'wb') as f_mean_std:\n",
    "        pickle.dump(dictionary, f_mean_std)\n",
    "\n",
    "    # Subtracting calculated mean image from pre-processed datasets\n",
    "    x_train -= mean_image\n",
    "    x_validation -= mean_image\n",
    "    x_test -= mean_image\n",
    "\n",
    "    # Dividing then every dataset by standard deviation\n",
    "    x_train /= std\n",
    "    x_validation /= std\n",
    "    x_test /= std\n",
    "\n",
    "    # Transposing every dataset to make channels come first\n",
    "    x_train = x_train.transpose(0, 3, 1, 2)  # (59000, 1, 28, 28)\n",
    "    x_test = x_test.transpose(0, 3, 1, 2)  # (10000, 1, 28, 28)\n",
    "    x_validation = x_validation.transpose(0, 3, 1, 2)  # (10000, 1, 28, 28)\n",
    "\n",
    "    # Returning result as dictionary\n",
    "    d_processed = {'x_train': x_train, 'y_train': y_train,\n",
    "                   'x_validation': x_validation, 'y_validation': y_validation,\n",
    "                   'x_test': x_test, 'y_test': y_test}\n",
    "\n",
    "    # Returning dictionary\n",
    "    return d_processed\n",
    "\n",
    "\n",
    "# Loading whole data for preprocessing\n",
    "x_train = load_data('datasets/train-images-idx3-ubyte.gz', 60000)\n",
    "y_train = load_labels('datasets/train-labels-idx1-ubyte.gz', 60000)\n",
    "x_test = load_data('datasets/t10k-images-idx3-ubyte.gz', 1000)\n",
    "y_test = load_labels('datasets/t10k-labels-idx1-ubyte.gz', 1000)\n",
    "\n",
    "# Preprocessing data\n",
    "data = pre_process_mnist(x_train, y_train, x_test, y_test)\n",
    "for i, j in data.items():\n",
    "    print(i + ':', j.shape)\n",
    "\n",
    "# x_train: (59000, 1, 28, 28)\n",
    "# y_train: (59000,)\n",
    "# x_validation: (1000, 1, 28, 28)\n",
    "# y_validation: (1000,)\n",
    "# x_test: (1000, 1, 28, 28)\n",
    "# y_test: (1000,)\n",
    "\n",
    "# Saving loaded and preprocessed data into 'pickle' file\n",
    "with open('data0.pickle', 'wb') as f:\n",
    "    pickle.dump(data, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
